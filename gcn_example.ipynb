{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c289fd8",
   "metadata": {},
   "source": [
    "一个简单的图：\n",
    "![graph](graph1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7f855",
   "metadata": {},
   "source": [
    "### 从节点、特征和度来刻画一个简单的图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d296927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd1c8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2],\n",
       "       [1, 1, 1],\n",
       "       [4, 4, 4],\n",
       "       [3, 3, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature matrix\n",
    "X = np.array([[2, 2, 2],   # 1\n",
    "              [1, 1, 1],   # 2\n",
    "              [4, 4, 4],   # 3\n",
    "              [3, 3, 3]])  # 4\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de5c299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1],\n",
       "       [1, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjacent matrix\n",
    "A = np.array([[0, 1, 0, 1],\n",
    "              [1, 0, 0, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 1, 1, 0]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa266aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# degree matrix\n",
    "D = np.diag(np.sum(A, axis=1))\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b105ba7",
   "metadata": {},
   "source": [
    "### 最简单的图神经网络\n",
    "目的：不断聚合邻居的信息来更新自身\n",
    "\n",
    "做法：将邻接矩阵跟结点特征矩阵相乘\n",
    "\n",
    "公式表达：$\\mathbf{H}^{k+1}=\\mathbf{A}\\mathbf{H}^{k}$\n",
    "\n",
    "注意：$\\mathbf{H}^{0}= \\mathbf{X}$\n",
    "\n",
    "验证：此方式做到的结果是用结点的邻居结点简单相加得到新的表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64eb8f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过一次迭代得到的结果\n",
      "[[4 4 4]\n",
      " [5 5 5]\n",
      " [3 3 3]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "print('经过一次迭代得到的结果')\n",
    "print(A.dot(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c14526",
   "metadata": {},
   "source": [
    "#### 以结点1为例验证更新后的结点1为结点2和结点4相加得到（2，4为其邻居）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "522d6fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4]\n",
      "[ True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(X[1] + X[3])\n",
    "print((X[1] + X[3]) == A.dot(X)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca282e3",
   "metadata": {},
   "source": [
    "### 简单的神经网络进阶1\n",
    "简单图神经网络总结：每一次迭代都用邻居结点更新自身\n",
    "\n",
    "注意：每一次迭代只是变化了结点信息，图的结构并未改变，即只更新$\\mathbf{X}$,$\\mathbf{A}$和$\\mathbf{D}$并未发生任何变化\n",
    "\n",
    "优点：考虑到了邻居信息\n",
    "\n",
    "缺点：丢失了自身信息\n",
    "\n",
    "解决问题：引入自身结点信息\n",
    "\n",
    "公式表达：\n",
    "$$\\tilde{\\mathbf{A}}=\\mathbf{A}+\\mathbf{I}_{N}$$\n",
    "$$\\mathbf{H}^{k+1}=\\tilde{\\mathbf{A}}\\mathbf{H}^{k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b273d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 1.],\n",
       "       [1., 1., 0., 1.],\n",
       "       [0., 0., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.diag(np.ones(4))\n",
    "hat_a = I + A\n",
    "hat_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1bb101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "考虑自身信息后经过一次迭代得到的结果：\n",
      "[[ 6.  6.  6.]\n",
      " [ 6.  6.  6.]\n",
      " [ 7.  7.  7.]\n",
      " [10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print('考虑自身信息后经过一次迭代得到的结果：')\n",
    "print(hat_a.dot(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9fd54",
   "metadata": {},
   "source": [
    "#### 以结点1为例验证更新后的结点1为结点2和结点4和自身相加得到（2，4为其邻居 + 自身）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cc09837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6]\n",
      "[ True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(X[1] + X[3] + X[0])\n",
    "print((X[1] + X[3] + X[0]) == hat_a.dot(X)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c71d79",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "目前做到的效果：即考虑到了邻居结点的信息，又考虑到了自身的信息\n",
    "\n",
    "进一步实验：真正迭代多次看具体的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea6c6f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代0次（最初）\n",
      "\n",
      "[[2 2 2]\n",
      " [1 1 1]\n",
      " [4 4 4]\n",
      " [3 3 3]]\n",
      "第1次\n",
      "\n",
      "[[ 6.  6.  6.]\n",
      " [ 6.  6.  6.]\n",
      " [ 7.  7.  7.]\n",
      " [10. 10. 10.]]\n",
      "第2次\n",
      "\n",
      "[[22. 22. 22.]\n",
      " [22. 22. 22.]\n",
      " [17. 17. 17.]\n",
      " [29. 29. 29.]]\n",
      "第3次\n",
      "\n",
      "[[73. 73. 73.]\n",
      " [73. 73. 73.]\n",
      " [46. 46. 46.]\n",
      " [90. 90. 90.]]\n",
      "第4次\n",
      "\n",
      "[[236. 236. 236.]\n",
      " [236. 236. 236.]\n",
      " [136. 136. 136.]\n",
      " [282. 282. 282.]]\n",
      "第5次\n",
      "\n",
      "[[754. 754. 754.]\n",
      " [754. 754. 754.]\n",
      " [418. 418. 418.]\n",
      " [890. 890. 890.]]\n"
     ]
    }
   ],
   "source": [
    "# 迭代5次，查看中间结果\n",
    "num_layer = 5\n",
    "H_wo = X\n",
    "print('迭代0次（最初）\\n')\n",
    "print(H_wo)\n",
    "for i in range(num_layer):\n",
    "    H_wo = hat_a.dot(H_wo)\n",
    "    print('第{}次\\n'.format(i + 1))\n",
    "    print(H_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24137f",
   "metadata": {},
   "source": [
    "### 趋势与问题\n",
    "趋势：随着迭代次数的增加，更新后的结点的感受野回越来越大，在这里则是更新后的结点由图中离自己相对更近的结点相加得到。 相应的， 更新后的结点特征值越来越大，和输入差距也越来越大，甚至随着迭代次数的增加，会溢出（越界）。其实，我们在做具体任务时只需要一个相对差距即可，原始数据和经过公平变化后的数据是几乎无差别的（归一化等）。\n",
    "\n",
    "注意：这个简单的图，其实迭代两次，已经能获得整个图中的所有信息\n",
    "\n",
    "解决办法：归一化\n",
    "\n",
    "其中一种方式：除以结点的度\n",
    "\n",
    "公式化：$$\\mathbf{H}^{k + 1} = \\mathbf{D}^{-1}\\tilde{\\mathbf{A}}\\mathbf{H}^{k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4d1fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代0次（最初）\n",
      "\n",
      "[[2 2 2]\n",
      " [1 1 1]\n",
      " [4 4 4]\n",
      " [3 3 3]]\n",
      "D^-1\n",
      "\n",
      "[[0.5        0.         0.         0.        ]\n",
      " [0.         0.5        0.         0.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.33333333]]\n",
      "第1次\n",
      "\n",
      "[[3.         3.         3.        ]\n",
      " [3.         3.         3.        ]\n",
      " [7.         7.         7.        ]\n",
      " [3.33333333 3.33333333 3.33333333]]\n",
      "第2次\n",
      "\n",
      "[[ 4.66666667  4.66666667  4.66666667]\n",
      " [ 4.66666667  4.66666667  4.66666667]\n",
      " [10.33333333 10.33333333 10.33333333]\n",
      " [ 5.44444444  5.44444444  5.44444444]]\n",
      "第3次\n",
      "\n",
      "[[ 7.38888889  7.38888889  7.38888889]\n",
      " [ 7.38888889  7.38888889  7.38888889]\n",
      " [15.77777778 15.77777778 15.77777778]\n",
      " [ 8.37037037  8.37037037  8.37037037]]\n",
      "第4次\n",
      "\n",
      "[[11.57407407 11.57407407 11.57407407]\n",
      " [11.57407407 11.57407407 11.57407407]\n",
      " [24.14814815 24.14814815 24.14814815]\n",
      " [12.97530864 12.97530864 12.97530864]]\n",
      "第5次\n",
      "\n",
      "[[18.0617284  18.0617284  18.0617284 ]\n",
      " [18.0617284  18.0617284  18.0617284 ]\n",
      " [37.12345679 37.12345679 37.12345679]\n",
      " [20.09053498 20.09053498 20.09053498]]\n"
     ]
    }
   ],
   "source": [
    "# 迭代5次，查看中间结果\n",
    "num_layer = 5\n",
    "H_w = X\n",
    "print('迭代0次（最初）\\n')\n",
    "print(H_w)\n",
    "D_1 = np.diag(1 / np.sum(A, axis=1)) \n",
    "print('D^-1\\n')\n",
    "print(D_1)\n",
    "for i in range(num_layer):\n",
    "    H_w = np.dot(D_1, hat_a.dot(H_w))\n",
    "    print('第{}次\\n'.format(i + 1))\n",
    "    print(H_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535342ea",
   "metadata": {},
   "source": [
    "### GCN\n",
    "变化1：采用对称归一化\n",
    "\n",
    "公式表达：\n",
    "$$\\mathbf{H}^{k + 1} = \\mathbf{D}^{-1/2}\\tilde{\\mathbf{A}}\\mathbf{D}^{-1/2}\\mathbf{H}^{k}$$\n",
    "\n",
    "变化2：引入可学习参数$\\mathbf{W}$\n",
    "\n",
    "公式表达：\n",
    "$$\\mathbf{H}^{k + 1} = \\mathbf{D}^{-1/2}\\tilde{\\mathbf{A}}\\mathbf{D}^{-1/2}\\mathbf{H}^{k}\\mathbf{W}^{k}$$\n",
    "\n",
    "变化3：引入激活函数$\\sigma$, 实现非线性\n",
    "\n",
    "公式表达：\n",
    "$$\\mathbf{H}^{k + 1} = \\sigma(\\mathbf{D}^{-1/2}\\tilde{\\mathbf{A}}\\mathbf{D}^{-1/2}\\mathbf{H}^{k}\\mathbf{W}^{k})$$\n",
    "\n",
    "最终：一般为两层\n",
    "公式表达：\n",
    "$$\\mathbf{Z} = f(\\mathbf{X}, \\mathbf{A}) = softmax(AReLu(\\mathbf{A}\\mathbf{X}\\mathbf{W}^{(0)})\\mathbf{W}^{(1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4c38e",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3fc6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据并保存\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae5350c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据的相关函数\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    # my add\n",
    "    #adj_temp1 = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    # print(adj_temp1)\n",
    "    print(\"未对称化之前的边数: \", adj.nnz)\n",
    "    # my add end    \n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    # my add\n",
    "    #adj_temp2 = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    # print(adj_temp2)\n",
    "    print(\"对称化之后的边数: \", adj.nnz)\n",
    "    # my add end\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    # my add\n",
    "    #adj_temp3 = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    # print(adj_temp3)\n",
    "    print(\"进一步标准化之后的边数: \", adj.nnz)\n",
    "    # my add end\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    #print(adj)\n",
    "    print(\"进一步转化为torch类型Coo格式后的边数: \", adj._nnz())\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c571b119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "未对称化之前的边数:  5429\n",
      "对称化之后的边数:  10556\n",
      "进一步标准化之后的边数:  13264\n",
      "进一步转化为torch类型Coo格式后的边数:  13264\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "# save data\n",
    "# torch.save({'adj': adj, 'features': features, 'labels': labels, 'idx_train': idx_train, 'idx_val': idx_val, 'idx_test': idx_test}, \"cora.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce19d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = torch.load(\"cora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54434be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of adj:  <class 'torch.Tensor'>\n",
      "The size of adj:  torch.Size([2708, 2708])\n",
      "The type of features:  <class 'torch.Tensor'>\n",
      "The size of features:  torch.Size([2708, 1433])\n",
      "The type of labels:  <class 'torch.Tensor'>\n",
      "The size of labels:  torch.Size([2708])\n",
      "The type of idx_train:  <class 'torch.Tensor'>\n",
      "The size of idx_train:  torch.Size([140])\n",
      "The type of idx_val:  <class 'torch.Tensor'>\n",
      "The size of idx_val:  torch.Size([300])\n",
      "The type of idx_test:  <class 'torch.Tensor'>\n",
      "The size of idx_test:  torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# 数据简单展示\n",
    "# 邻接矩阵\n",
    "adj = data[\"adj\"]\n",
    "print(\"The type of adj: \", type(adj))\n",
    "# print(adj)\n",
    "print(\"The size of adj: \", adj.size())\n",
    "# 特征矩阵\n",
    "features = data[\"features\"]\n",
    "print(\"The type of features: \", type(features))\n",
    "print(\"The size of features: \", features.size())\n",
    "# print(features)\n",
    "# 标签\n",
    "labels = data[\"labels\"]\n",
    "print(\"The type of labels: \", type(labels))\n",
    "print(\"The size of labels: \", labels.size())\n",
    "# print(labels)\n",
    "# 训练集索引\n",
    "idx_train = data[\"idx_train\"]\n",
    "print(\"The type of idx_train: \", type(idx_train))\n",
    "print(\"The size of idx_train: \", idx_train.size())\n",
    "# print(idx_train)\n",
    "# 验证集索引\n",
    "idx_val = data[\"idx_val\"]\n",
    "print(\"The type of idx_val: \", type(idx_val))\n",
    "print(\"The size of idx_val: \", idx_val.size())\n",
    "# print(idx_val)\n",
    "# 测试集索引\n",
    "idx_test = data[\"idx_test\"]\n",
    "print(\"The type of idx_test: \", type(idx_test))\n",
    "print(\"The size of idx_test: \", idx_test.size())\n",
    "# print(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6d1c0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes:  2708\n",
      "The dimension of feature:  1433\n",
      "类别数为： 7\n",
      "Label rate: 0.051698670605613\n"
     ]
    }
   ],
   "source": [
    "# 计算节点个数\n",
    "nodes = features.shape[0]\n",
    "print(\"The number of nodes: \", nodes)\n",
    "# 计算每个特征的维数\n",
    "d = features.shape[1]\n",
    "print(\"The dimension of feature: \", d)\n",
    "# 统计类别数\n",
    "print(\"类别数为：\", len(labels.unique()))\n",
    "# 标签率，特指训练的标签占总数的多少\n",
    "print(\"Label rate:\", len(idx_train) / nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c483a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求精确度函数\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b8ac549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9a0fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN 卷积层\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "                + str(self.in_features) + ' -> ' \\\n",
    "                + str(self.out_features) + ')'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26ce9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3494da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d7c5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 训练模型\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "    \n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e152257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模块\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss = {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dbdae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(fastmode=False, seed=42, epochs=200, lr=0.01, weight_decay=0.0005, hidden=16, dropout=0.5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 主函数\n",
    "# Parameters Setting\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])\n",
    "np.random.seed(args.seed)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71f37dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型与优化器\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4aa436a3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0450 acc_train: 0.0571 loss_val: 2.0439 acc_val: 0.0667 time: 0.0450s\n",
      "Epoch: 0002 loss_train: 2.0174 acc_train: 0.0571 loss_val: 2.0204 acc_val: 0.0667 time: 0.0090s\n",
      "Epoch: 0003 loss_train: 1.9961 acc_train: 0.0643 loss_val: 1.9987 acc_val: 0.0667 time: 0.0080s\n",
      "Epoch: 0004 loss_train: 1.9707 acc_train: 0.0714 loss_val: 1.9783 acc_val: 0.0700 time: 0.0070s\n",
      "Epoch: 0005 loss_train: 1.9495 acc_train: 0.0929 loss_val: 1.9593 acc_val: 0.1733 time: 0.0070s\n",
      "Epoch: 0006 loss_train: 1.9370 acc_train: 0.1786 loss_val: 1.9415 acc_val: 0.1567 time: 0.0070s\n",
      "Epoch: 0007 loss_train: 1.9120 acc_train: 0.2000 loss_val: 1.9247 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0008 loss_train: 1.8935 acc_train: 0.2000 loss_val: 1.9089 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0009 loss_train: 1.8898 acc_train: 0.2000 loss_val: 1.8946 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0010 loss_train: 1.8594 acc_train: 0.1929 loss_val: 1.8812 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0011 loss_train: 1.8569 acc_train: 0.2000 loss_val: 1.8685 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0012 loss_train: 1.8493 acc_train: 0.2071 loss_val: 1.8564 acc_val: 0.1567 time: 0.0090s\n",
      "Epoch: 0013 loss_train: 1.8307 acc_train: 0.2143 loss_val: 1.8446 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0014 loss_train: 1.8314 acc_train: 0.2143 loss_val: 1.8329 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0015 loss_train: 1.8039 acc_train: 0.2000 loss_val: 1.8215 acc_val: 0.1567 time: 0.0090s\n",
      "Epoch: 0016 loss_train: 1.7742 acc_train: 0.2357 loss_val: 1.8103 acc_val: 0.1567 time: 0.0090s\n",
      "Epoch: 0017 loss_train: 1.7639 acc_train: 0.2714 loss_val: 1.7995 acc_val: 0.1633 time: 0.0070s\n",
      "Epoch: 0018 loss_train: 1.7491 acc_train: 0.3357 loss_val: 1.7891 acc_val: 0.2800 time: 0.0080s\n",
      "Epoch: 0019 loss_train: 1.7799 acc_train: 0.3143 loss_val: 1.7789 acc_val: 0.4133 time: 0.0090s\n",
      "Epoch: 0020 loss_train: 1.7652 acc_train: 0.3571 loss_val: 1.7688 acc_val: 0.4367 time: 0.0080s\n",
      "Epoch: 0021 loss_train: 1.7468 acc_train: 0.4286 loss_val: 1.7585 acc_val: 0.4667 time: 0.0080s\n",
      "Epoch: 0022 loss_train: 1.7197 acc_train: 0.4357 loss_val: 1.7477 acc_val: 0.4533 time: 0.0080s\n",
      "Epoch: 0023 loss_train: 1.7113 acc_train: 0.4143 loss_val: 1.7366 acc_val: 0.3867 time: 0.0090s\n",
      "Epoch: 0024 loss_train: 1.7070 acc_train: 0.4000 loss_val: 1.7257 acc_val: 0.3667 time: 0.0100s\n",
      "Epoch: 0025 loss_train: 1.7121 acc_train: 0.4143 loss_val: 1.7151 acc_val: 0.3633 time: 0.0090s\n",
      "Epoch: 0026 loss_train: 1.6727 acc_train: 0.3714 loss_val: 1.7049 acc_val: 0.3633 time: 0.0070s\n",
      "Epoch: 0027 loss_train: 1.6819 acc_train: 0.3500 loss_val: 1.6952 acc_val: 0.3533 time: 0.0070s\n",
      "Epoch: 0028 loss_train: 1.6752 acc_train: 0.3429 loss_val: 1.6860 acc_val: 0.3533 time: 0.0090s\n",
      "Epoch: 0029 loss_train: 1.6493 acc_train: 0.3214 loss_val: 1.6771 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0030 loss_train: 1.6512 acc_train: 0.3143 loss_val: 1.6685 acc_val: 0.3533 time: 0.0080s\n",
      "Epoch: 0031 loss_train: 1.6158 acc_train: 0.3643 loss_val: 1.6603 acc_val: 0.3533 time: 0.0080s\n",
      "Epoch: 0032 loss_train: 1.6325 acc_train: 0.3643 loss_val: 1.6523 acc_val: 0.3567 time: 0.0090s\n",
      "Epoch: 0033 loss_train: 1.6090 acc_train: 0.3429 loss_val: 1.6443 acc_val: 0.3633 time: 0.0090s\n",
      "Epoch: 0034 loss_train: 1.5792 acc_train: 0.4000 loss_val: 1.6361 acc_val: 0.3633 time: 0.0070s\n",
      "Epoch: 0035 loss_train: 1.5960 acc_train: 0.3571 loss_val: 1.6276 acc_val: 0.3633 time: 0.0080s\n",
      "Epoch: 0036 loss_train: 1.5653 acc_train: 0.3714 loss_val: 1.6186 acc_val: 0.3667 time: 0.0080s\n",
      "Epoch: 0037 loss_train: 1.5688 acc_train: 0.3571 loss_val: 1.6092 acc_val: 0.3733 time: 0.0080s\n",
      "Epoch: 0038 loss_train: 1.5367 acc_train: 0.4214 loss_val: 1.5997 acc_val: 0.3800 time: 0.0090s\n",
      "Epoch: 0039 loss_train: 1.5313 acc_train: 0.4214 loss_val: 1.5901 acc_val: 0.3900 time: 0.0090s\n",
      "Epoch: 0040 loss_train: 1.5174 acc_train: 0.4500 loss_val: 1.5802 acc_val: 0.4067 time: 0.0080s\n",
      "Epoch: 0041 loss_train: 1.4939 acc_train: 0.4143 loss_val: 1.5699 acc_val: 0.4100 time: 0.0070s\n",
      "Epoch: 0042 loss_train: 1.4903 acc_train: 0.4071 loss_val: 1.5594 acc_val: 0.4133 time: 0.0080s\n",
      "Epoch: 0043 loss_train: 1.4668 acc_train: 0.4571 loss_val: 1.5483 acc_val: 0.4267 time: 0.0090s\n",
      "Epoch: 0044 loss_train: 1.4049 acc_train: 0.4714 loss_val: 1.5368 acc_val: 0.4333 time: 0.0070s\n",
      "Epoch: 0045 loss_train: 1.4203 acc_train: 0.4357 loss_val: 1.5252 acc_val: 0.4400 time: 0.0080s\n",
      "Epoch: 0046 loss_train: 1.4351 acc_train: 0.4786 loss_val: 1.5136 acc_val: 0.4500 time: 0.0080s\n",
      "Epoch: 0047 loss_train: 1.3992 acc_train: 0.4643 loss_val: 1.5019 acc_val: 0.4500 time: 0.0080s\n",
      "Epoch: 0048 loss_train: 1.3704 acc_train: 0.4786 loss_val: 1.4902 acc_val: 0.4500 time: 0.0080s\n",
      "Epoch: 0049 loss_train: 1.3574 acc_train: 0.4857 loss_val: 1.4786 acc_val: 0.4567 time: 0.0120s\n",
      "Epoch: 0050 loss_train: 1.3531 acc_train: 0.5000 loss_val: 1.4668 acc_val: 0.4633 time: 0.0090s\n",
      "Epoch: 0051 loss_train: 1.3415 acc_train: 0.4786 loss_val: 1.4548 acc_val: 0.4800 time: 0.0090s\n",
      "Epoch: 0052 loss_train: 1.3272 acc_train: 0.5500 loss_val: 1.4425 acc_val: 0.4967 time: 0.0080s\n",
      "Epoch: 0053 loss_train: 1.3169 acc_train: 0.5214 loss_val: 1.4296 acc_val: 0.5000 time: 0.0080s\n",
      "Epoch: 0054 loss_train: 1.2795 acc_train: 0.5286 loss_val: 1.4165 acc_val: 0.5067 time: 0.0090s\n",
      "Epoch: 0055 loss_train: 1.2957 acc_train: 0.5857 loss_val: 1.4031 acc_val: 0.5200 time: 0.0090s\n",
      "Epoch: 0056 loss_train: 1.2396 acc_train: 0.5571 loss_val: 1.3897 acc_val: 0.5233 time: 0.0080s\n",
      "Epoch: 0057 loss_train: 1.2546 acc_train: 0.5857 loss_val: 1.3767 acc_val: 0.5400 time: 0.0090s\n",
      "Epoch: 0058 loss_train: 1.1785 acc_train: 0.6143 loss_val: 1.3636 acc_val: 0.5467 time: 0.0090s\n",
      "Epoch: 0059 loss_train: 1.1964 acc_train: 0.5857 loss_val: 1.3503 acc_val: 0.5567 time: 0.0090s\n",
      "Epoch: 0060 loss_train: 1.1928 acc_train: 0.6286 loss_val: 1.3373 acc_val: 0.5600 time: 0.0070s\n",
      "Epoch: 0061 loss_train: 1.1786 acc_train: 0.6000 loss_val: 1.3247 acc_val: 0.5700 time: 0.0090s\n",
      "Epoch: 0062 loss_train: 1.1424 acc_train: 0.6214 loss_val: 1.3129 acc_val: 0.5833 time: 0.0080s\n",
      "Epoch: 0063 loss_train: 1.1258 acc_train: 0.6786 loss_val: 1.3016 acc_val: 0.6033 time: 0.0080s\n",
      "Epoch: 0064 loss_train: 1.1043 acc_train: 0.6643 loss_val: 1.2903 acc_val: 0.6133 time: 0.0080s\n",
      "Epoch: 0065 loss_train: 1.0965 acc_train: 0.7000 loss_val: 1.2791 acc_val: 0.6167 time: 0.0080s\n",
      "Epoch: 0066 loss_train: 1.1505 acc_train: 0.6929 loss_val: 1.2682 acc_val: 0.6333 time: 0.0080s\n",
      "Epoch: 0067 loss_train: 1.0571 acc_train: 0.7357 loss_val: 1.2567 acc_val: 0.6433 time: 0.0080s\n",
      "Epoch: 0068 loss_train: 1.0695 acc_train: 0.7429 loss_val: 1.2455 acc_val: 0.6433 time: 0.0090s\n",
      "Epoch: 0069 loss_train: 1.0465 acc_train: 0.7857 loss_val: 1.2341 acc_val: 0.6467 time: 0.0080s\n",
      "Epoch: 0070 loss_train: 1.0785 acc_train: 0.7500 loss_val: 1.2228 acc_val: 0.6533 time: 0.0060s\n",
      "Epoch: 0071 loss_train: 1.0869 acc_train: 0.7357 loss_val: 1.2121 acc_val: 0.6600 time: 0.0090s\n",
      "Epoch: 0072 loss_train: 1.0343 acc_train: 0.7786 loss_val: 1.2014 acc_val: 0.6667 time: 0.0090s\n",
      "Epoch: 0073 loss_train: 1.0019 acc_train: 0.7929 loss_val: 1.1910 acc_val: 0.6733 time: 0.0070s\n",
      "Epoch: 0074 loss_train: 0.9917 acc_train: 0.7929 loss_val: 1.1808 acc_val: 0.6733 time: 0.0080s\n",
      "Epoch: 0075 loss_train: 0.9782 acc_train: 0.7857 loss_val: 1.1709 acc_val: 0.6767 time: 0.0080s\n",
      "Epoch: 0076 loss_train: 0.9751 acc_train: 0.8214 loss_val: 1.1612 acc_val: 0.6800 time: 0.0080s\n",
      "Epoch: 0077 loss_train: 0.9637 acc_train: 0.8143 loss_val: 1.1519 acc_val: 0.6800 time: 0.0070s\n",
      "Epoch: 0078 loss_train: 0.9471 acc_train: 0.8000 loss_val: 1.1436 acc_val: 0.7033 time: 0.0100s\n",
      "Epoch: 0079 loss_train: 0.9485 acc_train: 0.8214 loss_val: 1.1353 acc_val: 0.7133 time: 0.0080s\n",
      "Epoch: 0080 loss_train: 0.9237 acc_train: 0.8286 loss_val: 1.1272 acc_val: 0.7133 time: 0.0080s\n",
      "Epoch: 0081 loss_train: 0.8731 acc_train: 0.8571 loss_val: 1.1195 acc_val: 0.7167 time: 0.0090s\n",
      "Epoch: 0082 loss_train: 0.8911 acc_train: 0.8643 loss_val: 1.1114 acc_val: 0.7300 time: 0.0090s\n",
      "Epoch: 0083 loss_train: 0.8990 acc_train: 0.8214 loss_val: 1.1027 acc_val: 0.7333 time: 0.0070s\n",
      "Epoch: 0084 loss_train: 0.8976 acc_train: 0.8286 loss_val: 1.0937 acc_val: 0.7367 time: 0.0070s\n",
      "Epoch: 0085 loss_train: 0.8947 acc_train: 0.8357 loss_val: 1.0845 acc_val: 0.7300 time: 0.0070s\n",
      "Epoch: 0086 loss_train: 0.8453 acc_train: 0.8357 loss_val: 1.0752 acc_val: 0.7367 time: 0.0070s\n",
      "Epoch: 0087 loss_train: 0.8671 acc_train: 0.8571 loss_val: 1.0665 acc_val: 0.7400 time: 0.0070s\n",
      "Epoch: 0088 loss_train: 0.8657 acc_train: 0.8214 loss_val: 1.0582 acc_val: 0.7400 time: 0.0080s\n",
      "Epoch: 0089 loss_train: 0.8434 acc_train: 0.8500 loss_val: 1.0504 acc_val: 0.7433 time: 0.0070s\n",
      "Epoch: 0090 loss_train: 0.8147 acc_train: 0.8429 loss_val: 1.0430 acc_val: 0.7567 time: 0.0080s\n",
      "Epoch: 0091 loss_train: 0.7967 acc_train: 0.8929 loss_val: 1.0358 acc_val: 0.7567 time: 0.0080s\n",
      "Epoch: 0092 loss_train: 0.8277 acc_train: 0.8286 loss_val: 1.0297 acc_val: 0.7567 time: 0.0080s\n",
      "Epoch: 0093 loss_train: 0.8089 acc_train: 0.8500 loss_val: 1.0234 acc_val: 0.7667 time: 0.0070s\n",
      "Epoch: 0094 loss_train: 0.8381 acc_train: 0.8857 loss_val: 1.0167 acc_val: 0.7700 time: 0.0090s\n",
      "Epoch: 0095 loss_train: 0.7700 acc_train: 0.8929 loss_val: 1.0099 acc_val: 0.7767 time: 0.0070s\n",
      "Epoch: 0096 loss_train: 0.7882 acc_train: 0.8786 loss_val: 1.0025 acc_val: 0.7767 time: 0.0070s\n",
      "Epoch: 0097 loss_train: 0.7420 acc_train: 0.8429 loss_val: 0.9950 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0098 loss_train: 0.7640 acc_train: 0.8571 loss_val: 0.9881 acc_val: 0.7700 time: 0.0080s\n",
      "Epoch: 0099 loss_train: 0.7432 acc_train: 0.8857 loss_val: 0.9816 acc_val: 0.7767 time: 0.0080s\n",
      "Epoch: 0100 loss_train: 0.7543 acc_train: 0.8429 loss_val: 0.9758 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0101 loss_train: 0.7576 acc_train: 0.8643 loss_val: 0.9700 acc_val: 0.7900 time: 0.0090s\n",
      "Epoch: 0102 loss_train: 0.6958 acc_train: 0.8929 loss_val: 0.9648 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0103 loss_train: 0.7083 acc_train: 0.8857 loss_val: 0.9593 acc_val: 0.7933 time: 0.0070s\n",
      "Epoch: 0104 loss_train: 0.7058 acc_train: 0.8929 loss_val: 0.9539 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0105 loss_train: 0.7235 acc_train: 0.8571 loss_val: 0.9486 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0106 loss_train: 0.7181 acc_train: 0.8500 loss_val: 0.9435 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0107 loss_train: 0.6933 acc_train: 0.8786 loss_val: 0.9381 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0108 loss_train: 0.6745 acc_train: 0.8929 loss_val: 0.9326 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0109 loss_train: 0.6905 acc_train: 0.8643 loss_val: 0.9276 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0110 loss_train: 0.7337 acc_train: 0.8786 loss_val: 0.9229 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0111 loss_train: 0.6555 acc_train: 0.8929 loss_val: 0.9177 acc_val: 0.7967 time: 0.0060s\n",
      "Epoch: 0112 loss_train: 0.6757 acc_train: 0.8857 loss_val: 0.9121 acc_val: 0.7933 time: 0.0080s\n",
      "Epoch: 0113 loss_train: 0.6740 acc_train: 0.8714 loss_val: 0.9064 acc_val: 0.7867 time: 0.0080s\n",
      "Epoch: 0114 loss_train: 0.6326 acc_train: 0.8929 loss_val: 0.9003 acc_val: 0.7900 time: 0.0070s\n",
      "Epoch: 0115 loss_train: 0.6346 acc_train: 0.8714 loss_val: 0.8942 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0116 loss_train: 0.6091 acc_train: 0.9000 loss_val: 0.8890 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0117 loss_train: 0.6210 acc_train: 0.8857 loss_val: 0.8840 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0118 loss_train: 0.6086 acc_train: 0.9143 loss_val: 0.8795 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0119 loss_train: 0.5988 acc_train: 0.9000 loss_val: 0.8757 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0120 loss_train: 0.6121 acc_train: 0.8929 loss_val: 0.8720 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0121 loss_train: 0.6174 acc_train: 0.8786 loss_val: 0.8684 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0122 loss_train: 0.6030 acc_train: 0.8857 loss_val: 0.8654 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0123 loss_train: 0.5907 acc_train: 0.9214 loss_val: 0.8616 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0124 loss_train: 0.6115 acc_train: 0.8929 loss_val: 0.8570 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0125 loss_train: 0.6111 acc_train: 0.9214 loss_val: 0.8525 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0126 loss_train: 0.5881 acc_train: 0.9071 loss_val: 0.8479 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0127 loss_train: 0.5946 acc_train: 0.9000 loss_val: 0.8446 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0128 loss_train: 0.5811 acc_train: 0.8786 loss_val: 0.8414 acc_val: 0.8033 time: 0.0080s\n",
      "Epoch: 0129 loss_train: 0.6225 acc_train: 0.8929 loss_val: 0.8383 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0130 loss_train: 0.6237 acc_train: 0.9000 loss_val: 0.8356 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0131 loss_train: 0.5695 acc_train: 0.8786 loss_val: 0.8326 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0132 loss_train: 0.5747 acc_train: 0.9214 loss_val: 0.8301 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0133 loss_train: 0.6027 acc_train: 0.8929 loss_val: 0.8274 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0134 loss_train: 0.5342 acc_train: 0.9286 loss_val: 0.8236 acc_val: 0.8000 time: 0.0081s\n",
      "Epoch: 0135 loss_train: 0.5602 acc_train: 0.8857 loss_val: 0.8190 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0136 loss_train: 0.5977 acc_train: 0.8929 loss_val: 0.8145 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0137 loss_train: 0.5693 acc_train: 0.8929 loss_val: 0.8102 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0138 loss_train: 0.5266 acc_train: 0.9071 loss_val: 0.8069 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0139 loss_train: 0.6015 acc_train: 0.8857 loss_val: 0.8048 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0140 loss_train: 0.5453 acc_train: 0.8929 loss_val: 0.8019 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0141 loss_train: 0.5935 acc_train: 0.9286 loss_val: 0.7993 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0142 loss_train: 0.4908 acc_train: 0.9357 loss_val: 0.7970 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0143 loss_train: 0.5519 acc_train: 0.8929 loss_val: 0.7946 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0144 loss_train: 0.4900 acc_train: 0.9143 loss_val: 0.7911 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0145 loss_train: 0.5458 acc_train: 0.9000 loss_val: 0.7876 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0146 loss_train: 0.5292 acc_train: 0.9071 loss_val: 0.7853 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0147 loss_train: 0.5310 acc_train: 0.9000 loss_val: 0.7829 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0148 loss_train: 0.5485 acc_train: 0.9000 loss_val: 0.7811 acc_val: 0.8033 time: 0.0070s\n",
      "Epoch: 0149 loss_train: 0.4892 acc_train: 0.9214 loss_val: 0.7804 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0150 loss_train: 0.5109 acc_train: 0.9286 loss_val: 0.7791 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0151 loss_train: 0.5277 acc_train: 0.9071 loss_val: 0.7765 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0152 loss_train: 0.5145 acc_train: 0.9214 loss_val: 0.7723 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0153 loss_train: 0.5132 acc_train: 0.9214 loss_val: 0.7664 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0154 loss_train: 0.4635 acc_train: 0.9357 loss_val: 0.7610 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0155 loss_train: 0.5363 acc_train: 0.9143 loss_val: 0.7561 acc_val: 0.8400 time: 0.0070s\n",
      "Epoch: 0156 loss_train: 0.4239 acc_train: 0.9571 loss_val: 0.7524 acc_val: 0.8367 time: 0.0090s\n",
      "Epoch: 0157 loss_train: 0.4798 acc_train: 0.9571 loss_val: 0.7499 acc_val: 0.8367 time: 0.0070s\n",
      "Epoch: 0158 loss_train: 0.4729 acc_train: 0.9286 loss_val: 0.7485 acc_val: 0.8400 time: 0.0070s\n",
      "Epoch: 0159 loss_train: 0.4679 acc_train: 0.9429 loss_val: 0.7480 acc_val: 0.8333 time: 0.0080s\n",
      "Epoch: 0160 loss_train: 0.4841 acc_train: 0.9071 loss_val: 0.7472 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0161 loss_train: 0.4844 acc_train: 0.9357 loss_val: 0.7460 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0162 loss_train: 0.5035 acc_train: 0.9143 loss_val: 0.7446 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0163 loss_train: 0.4434 acc_train: 0.9143 loss_val: 0.7425 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0164 loss_train: 0.4781 acc_train: 0.9286 loss_val: 0.7406 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0165 loss_train: 0.4989 acc_train: 0.9357 loss_val: 0.7387 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0166 loss_train: 0.4622 acc_train: 0.9357 loss_val: 0.7371 acc_val: 0.8300 time: 0.0100s\n",
      "Epoch: 0167 loss_train: 0.4854 acc_train: 0.9357 loss_val: 0.7360 acc_val: 0.8300 time: 0.0080s\n",
      "Epoch: 0168 loss_train: 0.4277 acc_train: 0.9571 loss_val: 0.7346 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0169 loss_train: 0.4320 acc_train: 0.9357 loss_val: 0.7323 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0170 loss_train: 0.4287 acc_train: 0.9643 loss_val: 0.7304 acc_val: 0.8233 time: 0.0070s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0171 loss_train: 0.4548 acc_train: 0.9429 loss_val: 0.7283 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0172 loss_train: 0.4575 acc_train: 0.9571 loss_val: 0.7270 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0173 loss_train: 0.4730 acc_train: 0.9143 loss_val: 0.7260 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0174 loss_train: 0.4659 acc_train: 0.9500 loss_val: 0.7259 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0175 loss_train: 0.4237 acc_train: 0.9214 loss_val: 0.7254 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0176 loss_train: 0.4188 acc_train: 0.9500 loss_val: 0.7248 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0177 loss_train: 0.4624 acc_train: 0.9214 loss_val: 0.7240 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0178 loss_train: 0.4730 acc_train: 0.9357 loss_val: 0.7235 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0179 loss_train: 0.4530 acc_train: 0.9571 loss_val: 0.7209 acc_val: 0.8100 time: 0.0070s\n",
      "Epoch: 0180 loss_train: 0.3857 acc_train: 0.9643 loss_val: 0.7190 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0181 loss_train: 0.4440 acc_train: 0.9357 loss_val: 0.7167 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0182 loss_train: 0.4317 acc_train: 0.9429 loss_val: 0.7148 acc_val: 0.8233 time: 0.0090s\n",
      "Epoch: 0183 loss_train: 0.4654 acc_train: 0.9643 loss_val: 0.7124 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0184 loss_train: 0.4106 acc_train: 0.9643 loss_val: 0.7105 acc_val: 0.8300 time: 0.0070s\n",
      "Epoch: 0185 loss_train: 0.4038 acc_train: 0.9643 loss_val: 0.7086 acc_val: 0.8300 time: 0.0090s\n",
      "Epoch: 0186 loss_train: 0.4196 acc_train: 0.9429 loss_val: 0.7074 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0187 loss_train: 0.4343 acc_train: 0.9286 loss_val: 0.7066 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0188 loss_train: 0.4164 acc_train: 0.9286 loss_val: 0.7067 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0189 loss_train: 0.4067 acc_train: 0.9357 loss_val: 0.7074 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0190 loss_train: 0.4107 acc_train: 0.9500 loss_val: 0.7089 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0191 loss_train: 0.4059 acc_train: 0.9500 loss_val: 0.7111 acc_val: 0.8267 time: 0.0090s\n",
      "Epoch: 0192 loss_train: 0.3828 acc_train: 0.9786 loss_val: 0.7119 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0193 loss_train: 0.4666 acc_train: 0.9357 loss_val: 0.7093 acc_val: 0.8267 time: 0.0081s\n",
      "Epoch: 0194 loss_train: 0.4046 acc_train: 0.9357 loss_val: 0.7058 acc_val: 0.8233 time: 0.0080s\n",
      "Epoch: 0195 loss_train: 0.4108 acc_train: 0.9500 loss_val: 0.7015 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0196 loss_train: 0.4274 acc_train: 0.9214 loss_val: 0.6977 acc_val: 0.8233 time: 0.0070s\n",
      "Epoch: 0197 loss_train: 0.4306 acc_train: 0.9429 loss_val: 0.6946 acc_val: 0.8267 time: 0.0080s\n",
      "Epoch: 0198 loss_train: 0.3845 acc_train: 0.9357 loss_val: 0.6921 acc_val: 0.8200 time: 0.0081s\n",
      "Epoch: 0199 loss_train: 0.3856 acc_train: 0.9500 loss_val: 0.6900 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0200 loss_train: 0.3818 acc_train: 0.9500 loss_val: 0.6888 acc_val: 0.8267 time: 0.0071s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.4040s\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b25a8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss = 0.7219 accuracy= 0.8320\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d63bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
